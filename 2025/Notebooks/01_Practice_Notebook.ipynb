{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52278fa",
   "metadata": {},
   "source": [
    "1. Write an SQL query to find the second highest salary from an employee table.\n",
    "\n",
    "2. How do you handle NULL values in SQL joins?\n",
    "\n",
    "3. Write a Python script to read a CSV file and load it into a DataFrame.\n",
    "\n",
    "4. How do you handle exceptions in Python using try-except blocks?\n",
    "\n",
    "5. In PySpark, how would you perform a join operation between two large DataFrames efficiently?\n",
    "\n",
    "6. Write a PySpark code to find the top 3 customers with the highest revenue per region.\n",
    "\n",
    "7. What is the difference between partitioning and bucketing in PySpark?\n",
    "\n",
    "8. How do you implement Slowly Changing Dimensions (SCD) in a data warehouse?\n",
    "\n",
    "9. Explain the concept of star schema and snowflake schema in data modeling.\n",
    "\n",
    "10. How would you design a fact table for an e-commerce platform?\n",
    "\n",
    "11. How do you build an ETL pipeline using Azure Data Factory?\n",
    "\n",
    "12. What are the different types of triggers in ADF and when to use them?\n",
    "\n",
    "13. Explain the architecture of Azure Databricks and its integration with Delta Lake.\n",
    "\n",
    "14. Write a PySpark code to process streaming data from Event Hub in Databricks.\n",
    "\n",
    "15. How do you optimize query performance in Azure Synapse Analytics?\n",
    "\n",
    "16. How would you design a data warehouse for a retail business using Synapse?\n",
    "\n",
    "17. What are the best practices for securing data in Azure Data Lake Storage?\n",
    "\n",
    "18. How do you manage access control and secrets using Azure Key Vault?\n",
    "\n",
    "19. Write a PySpark script to load data from ADLS into a Delta table.\n",
    "\n",
    "20. How do you implement data lineage and governance in Microsoft Purview?\n",
    "\n",
    "21. Build a real-time analytics pipeline using Event Hub, Stream Analytics, and Synapse.\n",
    "\n",
    "22. How would you handle late-arriving data in a batch ETL pipeline?\n",
    "\n",
    "23. Write an SQL query to calculate the customer churn rate over the last 6 months.\n",
    "\n",
    "24. How do you implement incremental data loading in ADF pipelines?\n",
    "\n",
    "25. Write a Python script to validate data quality and detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1838757",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--Write an SQL query to find the second highest salary from an employee table.\n",
    "\n",
    "with salary_ranking as (\n",
    "    Select EmployeeId, Salary, DENSE_RANK() OVER ( ORDER BY Salary DESC ) as salary_rank\n",
    "    from Employee\n",
    ")\n",
    "Select COALESCE(Salary, 0)\n",
    "from salary_ranking\n",
    "where salary_rank = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51b44d",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--How do you handle NULL values in SQL joins?\n",
    "\n",
    "-- If want to skip nulls\n",
    "\n",
    "SELECT * \n",
    "FROM table_a as a join table_b as b on a.col = b.col\n",
    "\n",
    "-- If want to include\n",
    "\n",
    "SELECT * \n",
    "FROM table_a as a join table_b as b on ( a.col = b.col or (a.col is null and b.col is null) ) \n",
    "\n",
    "-- If want to keep nulls after join, use left join\n",
    "\n",
    "SELECT * \n",
    "FROM table_a as a left join table_b as b on a.col = b.col \n",
    "where b.col is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc826945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a Python script to read a CSV file and load it into a DataFrame.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    return pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = input(\"Enter File Path : \").strip()\n",
    "    pandas_df = read_file(filepath)\n",
    "    print(pandas_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b63d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do you handle exceptions in Python using try-except blocks?\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    value1 = input(\"Insert First Value : \")\n",
    "    value2 = input(\"insert next value\")\n",
    "    try:\n",
    "        print(value1 / value2)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid Number format as : \", e)\n",
    "    except ZeroDivisionError as e:\n",
    "        print(\"Zero Division error as : \", e)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72700a3e",
   "metadata": {},
   "source": [
    "- In PySpark, how would you perform a join operation between two large DataFrames efficiently?\n",
    "\n",
    "    - Default join is sort merge , which is expensive because of shuffling\n",
    "    - We can use salting to perform the join\n",
    "    - First we need to check skew of dataframes\n",
    "    - Then we add salting keys\n",
    "    - increase number of shuffle partitions\n",
    "    - Increase parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SkewedJoin\").getOrCreate()\n",
    "\n",
    "# 1. Enable AQE + Skew handling FIRST (handles 80% of cases automatically)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")  # Tune based on cluster size\n",
    "\n",
    "# 2. Load tables\n",
    "large_df1 = spark.read.format(\"delta\").table(\"delta_table_1\")\n",
    "large_df2 = spark.read.format(\"delta\").table(\"delta_table_2\")\n",
    "\n",
    "# 3. **DETECT SKEW** - Critical first step\n",
    "def detect_skew(df, key_col, threshold=10):\n",
    "    skew_stats = df.groupBy(key_col).count().orderBy(\"count\", ascending=False).limit(20)\n",
    "    skew_stats.show()\n",
    "    total_rows = df.count()\n",
    "    max_rows = skew_stats.agg(max(\"count\")).collect()[0][0]\n",
    "    skew_ratio = max_rows * 100.0 / total_rows if total_rows > 0 else 0\n",
    "    print(f\"Max skew ratio for {key_col}: {skew_ratio:.2f}%\")\n",
    "    return skew_ratio > threshold\n",
    "\n",
    "skew1 = detect_skew(large_df1, \"join_key\")\n",
    "skew2 = detect_skew(large_df2, \"join_key\")\n",
    "\n",
    "# 4. Cache if large (AQE handles most spilling automatically)\n",
    "large_df1.cache()\n",
    "large_df2.cache()\n",
    "\n",
    "if skew1 or skew2:\n",
    "    print(\"Applying salting due to skew...\")\n",
    "    \n",
    "    # 5. **CORRECT SALTING** - Generate N salt values (e.g., 10)\n",
    "    salts = spark.range(1, 11).select(col(\"id\").cast(\"string\").alias(\"salt\"))\n",
    "    \n",
    "    # Salt large_df2 (smaller side usually) - explode to create 10x rows\n",
    "    large_df2_salted = large_df2.crossJoin(salts)\\\n",
    "        .withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    # Salt large_df1 - random salt per row (broadcast eligible if small)\n",
    "    large_df1_salted = large_df1.withColumn(\"salt\", floor(rand() * 10).cast(\"string\"))\\\n",
    "        .withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    # 6. Repartition (AQE may handle this too)\n",
    "    large_df1_salted = large_df1_salted.repartition(1000, \"salted_key\")\n",
    "    large_df2_salted = large_df2_salted.repartition(1000, \"salted_key\")\n",
    "    \n",
    "    # 7. Join on salted key\n",
    "    joined_df = large_df1_salted.join(\n",
    "        large_df2_salted, \n",
    "        on=\"salted_key\", \n",
    "        how=\"inner\"\n",
    "    ).drop(\"salted_key\", \"salt\")\n",
    "    \n",
    "else:\n",
    "    # No skew - direct join (AQE + broadcast if one side small)\n",
    "    joined_df = large_df1.join(large_df2, \"join_key\", \"inner\")\n",
    "\n",
    "# Verify & cleanup\n",
    "joined_df.explain()  # Check physical plan\n",
    "print(f\"Join completed. Rows: {joined_df.count()}\")\n",
    "large_df1.unpersist()\n",
    "large_df2.unpersist()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
