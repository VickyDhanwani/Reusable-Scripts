{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0af3ac3",
   "metadata": {},
   "source": [
    "Write PySpark code to create a DataFrame from a list of tuples containing (name, age, city) and display it.\n",
    "Example data: [(\"Alice\", 25, \"NYC\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 35, \"Chicago\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "data = [(\"Alice\", 25, \"NYC\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 35, \"Chicago\")]\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"city\", StringType())\n",
    "])\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9236254",
   "metadata": {},
   "source": [
    "Question 2 (Easy-Medium):\n",
    "Given a DataFrame with columns [\"product\", \"category\", \"price\"], write PySpark code to:\n",
    "\n",
    "Filter products where price > 100\n",
    "Group by category and count the number of products in each category\n",
    "Sort the results by count in descending order\n",
    "\n",
    "Sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fe093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType\n",
    "from pyspark.sql.functions import col, count as agg_count\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "schema = StructType([\n",
    "    StructField(\"product\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", IntegerType())\n",
    "])\n",
    "path = \"dbfs:/raw/data/products.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"True\").schema(schema).load(path)\n",
    "df = df.filter(col(\"price\") > 100)\n",
    "df = df.groupBy(\"category\").agg(agg_count(\"product\").alias(\"count_of_products\"))\n",
    "df = df.orderBy(col(\"count_of_products\").desc())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe49e71",
   "metadata": {},
   "source": [
    "Question 3 (Medium):\n",
    "Given two DataFrames:\n",
    "\n",
    "df_orders: columns [\"order_id\", \"customer_id\", \"amount\"]\n",
    "df_customers: columns [\"customer_id\", \"customer_name\", \"country\"]\n",
    "\n",
    "Write PySpark code to:\n",
    "\n",
    "Perform an inner join on customer_id\n",
    "Calculate the total amount spent per country\n",
    "Show only countries where total spending > 1000\n",
    "Display the results sorted by total amount in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import broadcast, sum as agg_sum, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"amount\", IntegerType()),\n",
    "])\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"customer_name\", StringType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "orders_path = \"dbfs:/raw/data/orders.parquet\"\n",
    "customers_path = \"dbfs:/raw/data/customers.parquet\"\n",
    "\n",
    "orders_df = spark.read.format(\"parquet\").schema(order_schema).load(orders_path)\n",
    "customer_df = spark.read.format(\"parquet\").schema(customer_schema).load(customers_path)\n",
    "\n",
    "joined_df = orders_df.join(broadcast(customer_df), \"customer_id\", \"inner\")\n",
    "df = joined_df.groupBy(\"country\").agg(agg_sum(\"amount\").alias(\"total_amount\"))\n",
    "df = df.filter(col(\"total_amount\") > 1000).orderBy(col(\"total_amount\").desc())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b81d9e",
   "metadata": {},
   "source": [
    "Write PySpark code to find duplicate records based on multiple columns. Given a DataFrame with columns [\"email\", \"phone\", \"name\"], identify and display all rows where the combination of email AND phone appears more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10367b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "from pyspark.sql.functions import col, concat, count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "schema = StructType([\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"phone\", StringType()),\n",
    "    StructField(\"name\", StringType())\n",
    "])\n",
    "\n",
    "data_path = \"dbfs:/raw/data/records.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"True\").schema(schema).load(data_path)\n",
    "df = df.withColumn(\"key\", concat(col(\"email\"), col(\"phone\")))\n",
    "window_spec = Window.partitionBy(\"key\")\n",
    "df = df.withColumn(\"rec_count\", count(\"*\").over(window_spec))\n",
    "df = df.filter(col(\"rec_count\") > 1).drop(col(\"rec_count\")).drop(col(\"key\"))\n",
    "df = df.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
