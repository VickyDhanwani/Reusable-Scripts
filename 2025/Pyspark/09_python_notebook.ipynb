{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac230c2",
   "metadata": {},
   "source": [
    "You have two DataFrames:\n",
    "df_sales: sale_id, product_id, sale_date, amount\n",
    "df_products: product_id, product_name, category\n",
    "Write PySpark code to:\n",
    "\n",
    "Join the two DataFrames on product_id\n",
    "Filter for sales from the last 30 days (assume today's date using current_date())\n",
    "Calculate the total sales amount per category\n",
    "Find the top 3 categories by total sales\n",
    "Include a column showing each category's percentage of total sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType\n",
    "from pyspark.sql.functions import col, date_sub, broadcast, current_date, round, sum as agg_sum, lit\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice_questions\").getOrCreate()\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"sale_date\", DateType()),\n",
    "    StructField(\"amount\", FloatType())\n",
    "])\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "sales_df = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", \"True\").load(\"dfbs:/raw/data/sales.csv\")\n",
    "products_df = spark.read.format(\"parquet\").schema(products_df).load(\"dbfs:/raw/data/products.parquet\")\n",
    "\n",
    "sales_df = sales_df.filter(col(\"sale_date\") < date_sub(current_date(), -30)) # filter early for predicate pushdown\n",
    "joined_df = sales_df.join(broadcast(products_df), \"product_id\", \"inner\")\n",
    "joined_df = joined_df.select(\"category\", \"amount\") # predicate pushdown\n",
    "\n",
    "joined_df = joined_df.groupBy(\"category\").agg(agg_sum(\"amount\").alias(\"total_sales\"))\n",
    "total_sale_sum = joined_df.agg(agg_sum(\"total_sales\")).collect()[0][0]\n",
    "window_spec = Window.orderBy(col(\"total_sales\").desc())\n",
    "\n",
    "\n",
    "ranked_df = joined_df.withColumn(\"percentage_sum\", round(col(\"total_sales\") * 100 / lit(total_sale_sum) , 2))\n",
    "ranked_df = joined_df.dense_rank().over(window_spec).alias(\"category_rank\")\n",
    "top_df = ranked_df.filter(col(\"category_rank\") <= 3)\n",
    "top_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced11a7",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "You have a DataFrame: employee_id, department, salary, hire_date\n",
    "Write PySpark code to:\n",
    "\n",
    "For each department, assign a rank to employees based on salary (highest salary = rank 1)\n",
    "Add a column showing the salary difference between each employee and the next highest-paid employee in their department\n",
    "Add a column showing each employee's salary percentile within their department (use percent_rank)\n",
    "Filter to show only employees in the top 2 ranks per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import  SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType, StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import col, percent_rank, lead\n",
    "spark  = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"employee_id\", StringType()),\n",
    "        StructField(\"department\", StringType()),\n",
    "        StructField(\"salary\", FloatType()),\n",
    "        StructField(\"hire_date\", DateType()),\n",
    "\n",
    "    ]\n",
    ")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"True\").schema(schema).load(\"dbfs:/raw/data/employee.csv\")\n",
    "\n",
    "department_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df = df.dense_rank().over(department_spec).alias(\"salary_rank\")\n",
    "df = df.withColumn(\"salary_diff\",col(\"salary\") - lead(\"salary\", 1).over(department_spec))\n",
    "df = df.withColumn(\"department_percentile_rank\", percent_rank().over(department_spec))\n",
    "df = df.filter(col(\"salary_rank\") <= 2).orderBy(\"department\", \"salary_rank\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392a4b9",
   "metadata": {},
   "source": [
    "You have a DataFrame: transaction_id, user_id, transaction_date, amount, transaction_type (values: 'credit' or 'debit')\n",
    "\n",
    "Write PySpark code to:\n",
    "\n",
    "Calculate running balance for each user (credits add, debits subtract) ordered by transaction_date\n",
    "Identify users who had negative balance at any point\n",
    "For these users, calculate: total transactions, total credits, total debits, and minimum balance reached\n",
    "Sort by minimum balance (most negative first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0500bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, FloatType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as agg_sum, when, count, min, lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType()),\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"transaction_date\", DateType()),\n",
    "    StructField(\"amount\", FloatType()),\n",
    "    StructField(\"transaction_type\", StringType()),\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"delta\").schema(schema).load(\"transactions\")\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"transaction_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"signed_amount\", \n",
    "                   when(col(\"transaction_type\") == \"credit\", col(\"amount\"))\n",
    "                   .otherwise(-col(\"amount\")))\n",
    "df = df.withColumn(\"running_balance\", agg_sum(\"signed_amount\").over(window_spec))\n",
    "# Find users with negative balance at any point\n",
    "negative_balance_users = df.filter(col(\"running_balance\") < 0).select(\"user_id\").distinct()\n",
    "\n",
    "# Filter to only those users\n",
    "df= df.join(negative_balance_users, \"user_id\", \"inner\")\n",
    "\n",
    "df = df.groupBy(\"user_id\").agg(count(\"*\").alias(\"total_transactions\"), agg_sum(when(col(\"transaction_type\") == \"credit\", col(\"amount\")).otherwise(lit(0))).alias(\"total_credits\"), agg_sum(when(col(\"transaction_type\") == \"debit\", col(\"amount\")).otherwise(lit(0))).alias(\"total_debits\"), min(\"running_balance\").alias(\"minimum_balance\"))\n",
    "df = df.orderBy(col(\"minimum_balance\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c2241",
   "metadata": {},
   "source": [
    "You have a DataFrame with columns: product_id, category, price, stock_quantity\n",
    "Write PySpark code to:\n",
    "\n",
    "Find products where stock_quantity is null and replace with 0\n",
    "Create a new column stock_status that shows:\n",
    "\n",
    "\"Out of Stock\" if stock_quantity = 0\n",
    "\"Low Stock\" if stock_quantity < 10\n",
    "\"In Stock\" if stock_quantity >= 10\n",
    "\n",
    "\n",
    "Filter for products in \"Electronics\" or \"Computers\" category\n",
    "Show results ordered by price descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83de385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=[\"stock_quantity\"]).withColumn(\"stock_status\", when(col(\"stock_quantity\") == 0, \"Out of Stock\").otherwise(when(col(\"stock_quantity\") < 10, \"Low Stock\").otherwise(\"In Stock\")))\n",
    "df = df.filter(col(\"product_id\") == \"Electronics\" | col(\"product_id\") == \"Computers\").orderBy(col(\"price\").desc())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cfb5f",
   "metadata": {},
   "source": [
    "orders_df: order_id, customer_id, order_date, total_amount\n",
    "customers_df: customer_id, customer_name, city, registration_date\n",
    "Write PySpark code to:\n",
    "\n",
    "Perform a left join to get all orders with customer details\n",
    "Find customers who have placed more than 3 orders\n",
    "For these customers, calculate their average order amount\n",
    "Show customer_name, city, order_count, and avg_order_amount\n",
    "Sort by order_count descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = orders_df.join(customers_df, \"customer_id\", \"left\")\n",
    "joined_df = joined_df.groupBy(\"customer_id\", \"customer_name\", \"city\").agg(count(\"*\").alias(\"order_count\"), round(avg(\"total_amount\"), 2).alias(\"avg_order_amount\"))\n",
    "joined_df = joined_df.filter(col(\"order_count\") > 3).orderBy(col(\"order_count\").desc()).select(\"customer_name\", \"city\", \"avg_order_amount\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2a963",
   "metadata": {},
   "source": [
    "Write PySpark code to:\n",
    "\n",
    "Calculate 7-day moving average of sales_amount for each product-region combination\n",
    "Pivot the data to show regions as columns with their moving averages\n",
    "Filter to show only the last 30 days of data (use datediff and current_date)\n",
    "Round moving averages to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222d7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
