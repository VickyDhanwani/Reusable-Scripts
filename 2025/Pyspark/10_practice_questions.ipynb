{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626178fd",
   "metadata": {},
   "source": [
    "Scenario: You have a DataFrame with columns user_id, product_id, and purchase_date.\n",
    "Question: Write PySpark code to find the count of distinct users who made purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"purchase_date\", DateType())\n",
    "])\n",
    "path = \"dbfs:/raw/data/products.csv\"\n",
    "df = spark.read.format(\"csv\").schema(schema).option(\"header\", \"True\").load(path)\n",
    "\n",
    "distinct_users = df.select(\"user_id\").distinct().count()\n",
    "print(distinct_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e3c19",
   "metadata": {},
   "source": [
    "Scenario: You have a DataFrame with user transactions:\n",
    "python\n",
    "# +-------+----------+--------+\n",
    "# |user_id|product_id| amount |\n",
    "# +-------+----------+--------+\n",
    "# |  101  |   501    |  50.0  |\n",
    "# |  102  |   502    |  75.0  |\n",
    "# |  101  |   503    |  30.0  |\n",
    "# |  103  |   501    |  60.0  |\n",
    "# |  102  |   504    |  45.0  |\n",
    "# +-------+----------+--------+\n",
    "```\n",
    "\n",
    "**Question:** Write PySpark code to:\n",
    "1. Calculate the total amount spent by each user\n",
    "2. Sort the results by total amount in descending order\n",
    "3. Show only the top 3 users\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "+-------+------------+\n",
    "|user_id|total_amount|\n",
    "+-------+------------+\n",
    "|  101  |   80.0     |\n",
    "|  102  |  120.0     |\n",
    "|  103  |   60.0     |\n",
    "+-------+------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import sum as agg_sum, col\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"amount\", FloatType())\n",
    "])\n",
    "path = \"dbfs:/raw/data/users.csv\"\n",
    "df = spark.read.format(\"csv\").schema(schema).option(\"header\",\"True\").load(path)\n",
    "df = df.groupBy(\"user_id\").agg(agg_sum(\"amount\").alias(\"total_amount\"))\n",
    "df = df.orderBy(col(\"total_amount\").desc()).limit(3)\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b706fe",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** Write PySpark code to find employees whose salary is **above the average salary of their department**.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "+------+-----------+----------+--------+--------------+\n",
    "|emp_id|   name    |department| salary |dept_avg_sal  |\n",
    "+------+-----------+----------+--------+--------------+\n",
    "| 3    |  Charlie  |    IT    | 90000  |  85000.0     |\n",
    "| 4    |  David    |    HR    | 65000  |  62500.0     |\n",
    "| 5    |  Eve      |  Finance | 75000  |  72500.0     |\n",
    "+------+-----------+----------+--------+--------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af553398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import round, avg, col\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "df_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"department\", StringType()),\n",
    "    StructField(\"salary\", FloatType())\n",
    "])\n",
    "path = \"dbfs:/raw/data/employee.csv\"\n",
    "df = spark.read.format(\"csv\").schema(df_schema).option(\"header\", \"True\").load(path)\n",
    "avg_window_spec = Window.partitionBy(\"department\")\n",
    "df = df.withColumn(\"dept_avg_sal\", round(avg(col(\"salary\")).over(avg_window_spec), 2))\n",
    "df = df.filter(col(\"salary\") > col(\"dept_avg_sal\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a027bd5",
   "metadata": {},
   "source": [
    "**Question:** Write PySpark code to calculate a **7-day moving average of revenue** for each user, ordered by date.\n",
    "\n",
    "**Requirements:**\n",
    "1. Calculate the moving average including the current day and previous 6 days (7 days total)\n",
    "2. If there are fewer than 7 days available, calculate the average with available days\n",
    "3. Round the result to 2 decimal places\n",
    "4. Order by user_id and date\n",
    "\n",
    "**Expected Output (example):**\n",
    "```\n",
    "+-------+------------+--------+------------------+\n",
    "|user_id|   date     |revenue |moving_avg_7day   |\n",
    "+-------+------------+--------+------------------+\n",
    "|  101  | 2024-01-01 |  100   |     100.00       |\n",
    "|  101  | 2024-01-02 |  150   |     125.00       |\n",
    "|  101  | 2024-01-03 |  200   |     150.00       |\n",
    "|  101  | 2024-01-05 |  180   |     157.50       |\n",
    "|  102  | 2024-01-01 |   80   |      80.00       |\n",
    "|  102  | 2024-01-02 |  120   |     100.00       |\n",
    "|  103  | 2024-01-01 |   90   |      90.00       |\n",
    "+-------+------------+--------+------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, FloatType\n",
    "from pyspark.sql.functions import col, round, avg\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"date\", DateType()),\n",
    "    StructField(\"revenue\", IntegerType())\n",
    "])\n",
    "path = \"dbfs:/raw/data/revenue.csv\"\n",
    "df = spark.read.format(\"csv\").schema(schema).option(\"header\", \"True\").load(path)\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "df = df.withColumn(\"moving_avg_7day\", round(avg(\"revenue\").over(window_spec), 2))\n",
    "df = df.orderBy(col(\"user_id\"), col(\"date\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9597a",
   "metadata": {},
   "source": [
    "python\n",
    "# +-------+---------------------+\n",
    "# |user_id|     login_time      |\n",
    "# +-------+---------------------+\n",
    "# |  101  | 2024-01-01 09:00:00 |\n",
    "# |  101  | 2024-01-01 09:05:00 |\n",
    "# |  101  | 2024-01-01 09:40:00 |\n",
    "# |  101  | 2024-01-02 10:00:00 |\n",
    "# |  102  | 2024-01-01 08:00:00 |\n",
    "# |  102  | 2024-01-01 08:45:00 |\n",
    "# |  102  | 2024-01-01 09:50:00 |\n",
    "# +-------+---------------------+\n",
    "```\n",
    "\n",
    "**Question:** Identify **user sessions** where a new session starts if there's a gap of **more than 30 minutes** between consecutive logins for the same user.\n",
    "\n",
    "**Requirements:**\n",
    "1. Assign a unique `session_id` to each session\n",
    "2. Calculate the `session_start` time (first login of the session)\n",
    "3. Calculate the `session_end` time (last login of the session)\n",
    "4. Count the number of logins in each session\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "+-------+----------+---------------------+---------------------+-------------+\n",
    "|user_id|session_id|    session_start    |     session_end     |login_count  |\n",
    "+-------+----------+---------------------+---------------------+-------------+\n",
    "|  101  |    1     | 2024-01-01 09:00:00 | 2024-01-01 09:05:00 |      2      |\n",
    "|  101  |    2     | 2024-01-01 09:40:00 | 2024-01-01 09:40:00 |      1      |\n",
    "|  101  |    3     | 2024-01-02 10:00:00 | 2024-01-02 10:00:00 |      1      |\n",
    "|  102  |    1     | 2024-01-01 08:00:00 | 2024-01-01 08:45:00 |      2      |\n",
    "|  102  |    2     | 2024-01-01 09:50:00 | 2024-01-01 09:50:00 |      1      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, lag, unix_timestamp, when, sum, min, max, count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"login_time\", TimestampType())\n",
    "])\n",
    "path = \"dbfs:/raw/data/login_times.csv\"\n",
    "df = spark.read.format(\"csv\").schema(schema).option(\"header\", \"True\").load(path)\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"login_time\")\n",
    "df = df.withColumn(\"prevLoginTime\", lag(\"login_time\", 1).over(window_spec))\n",
    "df = df.withColumn(\"time_diff_min\", (unix_timestamp(col(\"login_time\")) - unix_timestamp(col(\"prevLoginTime\"))) / 60)\n",
    "df = df.withColumn(\"is_new_session\", when(((col(\"time_diff_min\") > 30) | (col(\"time_diff_min\").isNull())) , 1).otherwise(0))\n",
    "df = df.withColumn(\"session_id\", sum(\"is_new_session\").over(window_spec))\n",
    "df = df.groupBy(\"user_id\", \"session_id\").agg(min(\"login_time\").alias(\"session_start\"),\n",
    "                                             max(\"login_time\").alias(\"session_end\"),\n",
    "                                             count(\"login_time\").alias(\"login_count\")\n",
    "                                            )\n",
    "df = df.orderBy(\"user_id\", \"session_id\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038f513",
   "metadata": {},
   "source": [
    "# +--------+----------+------------+--------+\n",
    "# |order_id|customer_id|order_date |amount  |\n",
    "# +--------+----------+------------+--------+\n",
    "# |  1001  |   101    | 2024-01-15 |  250.0 |\n",
    "# |  1002  |   102    | 2024-01-16 |  180.0 |\n",
    "# |  1003  |   101    | 2024-01-17 |  320.0 |\n",
    "# |  1004  |   999    | 2024-01-18 |  150.0 |  # Orphan record\n",
    "# |  1005  |   103    | 2024-01-19 |  400.0 |\n",
    "# |  1006  |   101    | 2024-01-20 |  275.0 |\n",
    "# +--------+----------+------------+--------+\n",
    "\n",
    "# +----------+-----------+----------+\n",
    "# |customer_id|   name   | segment  |\n",
    "# +----------+-----------+----------+\n",
    "# |   101    |  Alice   | Premium  |\n",
    "# |   102    |  Bob     | Standard |\n",
    "# |   103    |  Charlie | Premium  |\n",
    "# |   104    |  David   | Standard |  # No orders\n",
    "# +----------+-----------+----------+\n",
    "```\n",
    "\n",
    "**Question:** Write PySpark code to:\n",
    "\n",
    "1. **Identify and report data quality issues:**\n",
    "   - Orders with no matching customer (orphan orders)\n",
    "   - Customers with no orders\n",
    "   \n",
    "2. **Create a clean dataset** that includes:\n",
    "   - All valid orders with customer information\n",
    "   - Calculate total order amount per customer\n",
    "   - Calculate number of orders per customer\n",
    "   - Rank customers by total amount within each segment\n",
    "\n",
    "3. **Output two DataFrames:**\n",
    "   - `data_quality_report`: Shows all data quality issues\n",
    "   - `clean_customer_summary`: Final clean dataset with rankings\n",
    "\n",
    "**Expected Output 1 - Data Quality Report:**\n",
    "```\n",
    "+-------------+----------+-------------+\n",
    "|  issue_type |entity_id | entity_type |\n",
    "+-------------+----------+-------------+\n",
    "| orphan_order|   1004   |   order     |\n",
    "| no_orders   |   104    |  customer   |\n",
    "+-------------+----------+-------------+\n",
    "```\n",
    "\n",
    "**Expected Output 2 - Clean Customer Summary:**\n",
    "```\n",
    "+----------+---------+---------+-------------+------------+-------------+\n",
    "|customer_id| name   | segment |order_count  |total_amount|segment_rank |\n",
    "+----------+---------+---------+-------------+------------+-------------+\n",
    "|   101    | Alice  | Premium |      3      |   845.0    |      1      |\n",
    "|   103    | Charlie| Premium |      1      |   400.0    |      2      |\n",
    "|   102    | Bob    |Standard |      1      |   180.0    |      1      |\n",
    "+----------+---------+---------+-------------+------------+-------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce5ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
